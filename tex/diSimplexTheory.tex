% LaTeX source for the diSimplexTheory document
%

\documentclass[a4paper,openany]{amsbook}
\usepackage{disitt}
\usepackage{disitt-symbols}

\begin{document}
\frontmatter
\sloppy

\title[DiSimplicial Theory]{Reality of Mathematics: Directed Simplicial Theory}
\input{frontMatter}
\subjclass[2010]{Primary unknown; Secondary unknown} %
\keywords{Keyword one, keyword two etc.}%

\begin{abstract}
To be able to provide a mathematical theory of Reality, one must first 
address the question: \textit{How ``Real'' is Mathematics?}
\end{abstract} 
\maketitle 
\tableofcontents 
\mainmatter

\begin{quotation}
In the places I go there are things that I see\\
That I \emph{never} could spell if I stopped with the Z.\\
\begin{quote}
\textit{On Beyond Zebra} by Dr Seuss.
\end{quote}
\end{quotation}

\section{Introduction}

The aim of this cycle of papers is to provide a rigorous mathematical theory of
Reality. This first paper provides \emph{an} answer to the question: \emph{How
``Real'' is Mathematics?}, that is, \emph{what is the validity of the
mathematics used to provide a mathematical theory of Reality?}

Any answer to this question, which forms part of this cycle of papers, must of
necessity be circular: what is the Reality of the mathematics which provides a
mathematical description of Reality? How Real is mathematics? If it is not
Real/valid, then is Reality itself Real/valid?

To break this circularity, we take the pragmatic approach, bastardising
Descartes, and state: \emph{I make marks in the sand, so I exist}.

Any discussion of the `Reality' or `Validity' of Mathematics is essentially a
discussion of the \textit{Foundations of Mathematics}. Classically Mathematics
is founded upon some form of Set Theory, such as Zermelo-â€“Fraenkel set theory
with the axiom of choice (ZFC), expressed in the language of First Order
Predicate Logic. To avoid Russel's paradox, one form of the Liar Paradox, ZFC
explicitly includes an axiom forcing all sets to be well--founded. From a
Categorical point of view, this means that classical Set Theory is an
\emph{Algebraic} theory. From a computational point of view, this means that
classical Set Theory is `data'.

One of the primary aims of this cycle of papers, is to show that the Sciences
and Engineering, are better served by a Mathematics which focuses on spatially
distributed interacting \emph{processes}. This means that, from a Categorical
point of view, we are interested in \emph{Co-Algebraic} structures as the
Foundations of Mathematics.

Equally importantly, `Truth', as expressed by Aristotelian Logic, in the form of
First Order Predicate Logic, is less important then the ability to compute. We
will show that First Order Predicate Logic, as traditionally used in
Mathematics, is the \emph{internal} logic of \emph{the} Co-Algebraic Universe of
Mathematics. We will refer to this Co-Algebraic Universe as \emph{`Plato's
Universe'}, \Universe.

We base our foundations of mathematics on generalized computation \emph{rather
than} Logic.

Clearly, not all of classical Mathematics is (classically) `computable'.  For
example, the use of the (the unrestricted) Axiom of Choice, is un-computable by
a classical Turing Machine. This suggests that, to capture the whole of
classical Mathematics, we will need a more nuanced model of computation. The
critical point here is that classical mathematics is developed from an
\emph{omnipotent} point of view, yet as finite beings, we only have far more
limited computational abilities. In out best 'school Latin`, we christen beings
with these more limited abilities, as \emph{modipotent}.

The classical theory of computation typically investigates the relative power of
computation using turing machines \emph{extended} with oracles. We will instead
introduce computation whose power is controled by the algebraic `size' of its
available data structures. For each \emph{algebraic} cardinal, $\gamma$, there
is an associated computational power of a $\gamma$-modipotent being. The
unlimited computational power of an omnipotent being (a `god') corresponds to
the \emph{co-algebraic} cardinal.

The co-algebraic \emph{sub}universe of mathematics computable by an
$\omega$-modipotent being, such as ourselves, we will refer to as \emph{`Plato's
Playground'}.

One of our primary interests is in how \emph{`Plato's Playground'} fits `inside'
\emph{`Plato's universe'}. Clearly, $\omega$-modipotent beings can `dream' of,
and hence specify, structures which they can not construct using
$\omega$-computational tools. That is there are structures whose specifications
`exist' in Plato's playground but which can only be realized in Plato's
universe.

Often to really understand a topic, it is useful to develop the same ideas using
very different tools. After just over 100 years, it is past time to develop the
foundations of mathematics using different tools.  In our case using computation
rather than logic. However, not surprisingly, there will be rough
correspondences between the different sets of tools. Again, in our case, each
basic assumption in our computational theory of mathematics, corresponds to one
or more of the axioms of Set Theory. The essentiality of these correspondences
is what provides us with a deep understanding of what it \emph{means} to
\emph{found} mathematics.

How can we understand the shift in focus from a Logical foundation for
mathematics to a Computational foundation for mathematics? In the classical
Logical foundation of mathematics it is the study of the (algebraic) structure
of the logical implications of a set of axioms which is primary. Given these
sets of axioms we can then investigate the collection of \emph{models} which
satisfy the axioms. In the Computational foundations of mathematics, it is the
co-algebraic collections of \emph{constructed} models which is primary.  Given
these co-algebraic models there are then logical statements which distinguish,
test or verify the models.

\TODO{rework the following}

As will become obvious later in this cycle of papers, ``Reality'' is \emph{a}
model of \emph{a} higher topos.  Unfortunately, for finite beings \emph{inside}
Reality, we can only ever specify a possible collection of possible Realities. A
collection of Realities which are consistent with our current, finitely limited,
understanding, or measurement of ``Reality''.

Far more fundamental and far more important than propositions is the concept of
Causality.  I will argue, in later work, that the human brain, is a causality
inference engine. I will equally argue that the unification of Relativity and
Quantum Mechanics requires a careful understanding of the \emph{finite}
structure of causality.

As should become evident, developing the mathematics of the \emph{finite}
structure of causality, is most easily done when based upon Directed Simplicial
Theory rather than classical propositional/predicate calculus. Following the
lead of HoTT, propositional/predicate calculus, as well as set theory will be
specific ``universes'' of Directed Simplicial Type Theory.

The important point here, is that, while the mathematics of causality is
\emph{implicitly} ``part'' of classical logic, it is conceptually useful to make
the structure of causality \emph{explicit} in our foundations of mathematics. 
Inferring and manipulating causality is after all the main aim of all of the
sciences and engineering.

This book is devoted to giving the reader a more ``mathematical'' overview of
Directed Simplicial Theory.  It is meant to be read in companion with the very
much more detailed \textit{Directed Simplicial Theory Implemented in Haskell}.

\section{Building Plato's Wilderness}

Our explicit thesis is that Mathematics \emph{is} computation.

To begin, we must provide a model of computation. The classical models of
computability, such as Turing Machines, the Lambda Calculus and General
Recursive Functions, are all, more or less, about computational sequentiallity
and `time'. While the model we are about to propose will be equivilant to any of
the classical models, it is designed to be \emph{explicitly} about parallel
computation and `space--time'. Moreover, since classical mathematics is
mathematics from an omnipotent point of view, the computational model required
to capture classical mathematics, must \emph{explicitly} have unbounded
computational ability.

Since Mathematics \emph{is} computation, we, as mathematicians, write \emph{in}
a programming language. As in Computer Science, there are many programming
languages, each suited to a different paradigm, there will be numerous
mathamatical languages.

Again, in Theoretical Computer Science, given a programming language, the first
question is: \emph{How do we know what this language computes?} This question is
captured by the Denotational and Operational Semantics associated with a given
language. Plato's universe, \emph{the} Co-Algebraic universe of Mathematics,
is essentially, the fixed point of these Semantic descriptions.

When the dust settles, to define Plato's Universe, \Universe, we will need to
simultaineously define, via co--induction, the three structures of \Universe,
\Lists, \ListAutomorphisms:
%
\begin{align}
   \Universe          & \isomorphic \arrow{\ListAutomorphisms}{\Universe} & \text{Plato's Universe} \\
   \Lists             & \isomorphic \one + \Universe \times \Lists        & \text{(Gernalized) Lists} \\
   \ListAutomorphisms & \isomorphic \arrow{\Lists}{\Lists}                & \text{(Gernalized) List Automorphisms}
\end{align}
%
Unfortunately, since we \emph{are} building the foundations of Mathematics, we,
as yet, have no definitions of co--induction, $\isomorphic$, \one, $+$, $\times$,
$\arrow{\cdot}{\cdot}$, let alone, \Universe, \Lists\ or even \ListAutomorphisms.

Again, when the dust settles, Lists, \Lists, is \emph{an} \emph{implementation}
of the classical concept of the Ordinals, \Ordinal, and the isomorphism objects
of the Lists modulo List Automorphisms, \ListAutomorphisms, is \emph{an}
\emph{implementation} of the classical concepts of the Cardinals, \Cardinal. In
deed we will, eventually, make these explicit definitions, however, at the
moment, we are getting ahead of our current (minimal) abilities.

\subsection{Syntax: Marks in the sand}

We \emph{explicity} place \emph{no limits} on the size of the syntax. This means
we make no assumption about the numbers of symbols which can be used, the number
of symbols which can be `written', nor the length of time which might be used to
`write these symbols down'. In each case we explicitly allow for `transfinite
numbers' (when of course we have suitably defined `transfinite ordinals'). Note
that, in contrast to classical set theory, to `compute' any large ordinals,
insteasd of merely assume they exist, we do indeed require large ordinal lengths
of time to write down large ordinal numbers of symbols.

DiSimplicial structures, which are instances of Plato's universe, are highly
structured objects with complex referential structure. There are two
distinct types of references that we will use: 
%
\begin{itemize}
%
\item Specific known references, \define{names}{}, used to cross-link known
(sub)structures in a given diSimplicial structure, and 
%
\item Specific unknown references, \define{variables}{}, used to denote
(sub)structures we might be in the process of building and/or which we assume
might exist. 
%
\end{itemize}
%


\subsection{Denotational Semantics}

\subsection{Operational Semantics}



\begin{definition}
content...
\end{definition}

\subsection{motivation}

A sketch of nearly random ideas relating to the base of the foundations of
mathematics via computational complexity.

start by looking at finite computation as above BUT we need to generalize lists
to encompase the supremum operator. See chapter 2 and 3 of Algebraic Set Theory.
compare the trees used to prove the existence of ZF-Algebras (chapter 3) with
those used to help motivate Aczel's Non-Well-Founded sets. compare these with
Vopenka's (rigid graph/tree) principle in Adamek and Rosicky's book Locally
Presentable and Accessible Categories.

Vopenka's principle is described in Mathoverflow articles:
\verb|http://mathoverflow.net/questions/29302/reasons-to-believe-vopenkas-principle-huge-cardinals-are-consistent/29473#29473|
and
\verb|http://mathoverflow.net/questions/45602/can-vopenkas-principle-be-violated-definably/46538#46538|
and or Adamek and Rosicky's book as really delineating the ``correct'' boundary
between small and large sets. As such our theory \emph{should} ensure the
Vopenka's principle is honoured.

basically using AST's existence work, we can build the ordinals just in time (?)
to extend finite computers into transfinite computers. (See Moschovakis's paper
on definition of algorithms -- he does not go far enough and use co-algebraic
methods... but his computer tools do assume ordinal computation as we need to
do).

need to understand the relationship between implicit and explicit computation
--- see the similar dichotome in the lambda-calculus.

look for p =!= np in the categorical theorems which force the use of the
small/large boundary.

look at: lucas2001transfiniteRewrite,
dershowitzKaplanPlaisted1989transfiinteRewrite,
jacobs2011coalgebraicComputation

\bibliographystyle{amsalpha}
\bibliography{diSimplexTheory}

\end{document}

